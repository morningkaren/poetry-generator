{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the start of the notebook of a poem generator. 21,500 poems were scraped or collected off the internet, mainly from poemhunter.com and a Kaggle dataset of poems. The poems were cleaned and tokenized on the character level. Then, they were further pre-processed to be in the correct format to input into an LSTM model (a recurrent neural network). Then, the model was used to create a poetry generator. Given a seed of 100+ characters, a 500-character (can be changed) continuation of the poem was generated. \n",
    "\n",
    "2. <a href='#scraping'>Web Scraping</a>\n",
    "\n",
    "    2.1. <a href='#poemhunter'>Web Scraping Poemhunter.com</a>\n",
    "    \n",
    "    2.2. <a href='#180poems'>Web Scraping poetry 180</a>\n",
    "    \n",
    "    \n",
    "3. <a href='#clean'>Cleaning the Data</a>\n",
    "\n",
    "\n",
    "4. <a href='#100'>Create 100 sequence chunks</a>\n",
    "\n",
    "\n",
    "5. <a href='#1024'>Create batches of 1024 100 sequence chunks</a>\n",
    "    \n",
    "\n",
    "6. <a href='#lstm'>LSTM Model</a>\n",
    "\n",
    "\n",
    "7. <a href='#generator'>Poetry Generator</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping \n",
    "<a id='scraping'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T20:30:00.656099Z",
     "start_time": "2019-09-16T20:30:00.648303Z"
    }
   },
   "source": [
    "### Web scraping poemhunter.com\n",
    "<a id='poemhunter'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import beautiful soup and other modules\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.poemhunter.com/poem-topics/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "#check is response = 200.\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get url for each topic.\n",
    "def get_urls1(website):\n",
    "    page = requests.get(website)\n",
    "    data = page.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    list_of_urls = []\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"poems/\")}):\n",
    "        url = (link.get('href'))\n",
    "        list_of_urls.append(url)\n",
    "    return list_of_urls\n",
    "\n",
    "#url_list contains all urls to the different poem topics.\n",
    "url_list = get_urls1('https://www.poemhunter.com/poem-topics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get url in each topic (page 1 of 2 only)\n",
    "def get_urls2(element):\n",
    "    list_of_urls = []\n",
    "    #url_list[-12:] is not relevant urls.\n",
    "    for element in url_list[1:-12]:\n",
    "        page = requests.get('http://poemhunter.com'+f'{element}')\n",
    "        data = page.text\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(f\"{element}\")}):\n",
    "            url = (link.get('href'))\n",
    "            list_of_urls.append(url)\n",
    "    return list_of_urls\n",
    "\n",
    "url_list2 = get_urls2(url_list[1:-12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters out list of urls in url_list2 to get the relevant urls for page 1.\n",
    "regex = re.compile(\"https:\")\n",
    "filtered = [i for i in url_list2 if not regex.search(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save urls from page 1 of 2 into a pkl file\n",
    "pickling_on = open(\"url_page1.pkl\",\"wb\")\n",
    "pickle.dump(filtered, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get url in each topic (page 2)\n",
    "def get_urls3(element):\n",
    "    list_of_urls = []\n",
    "    for element in url_list[1:-12]:\n",
    "\n",
    "        page = requests.get('http://poemhunter.com/'+f'{element}' + 'page-2')\n",
    "        data = page.text\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(f\"{element}\"+ \"page-2\")}):\n",
    "            url = (link.get('href'))\n",
    "            list_of_urls.append(url)\n",
    "    return list_of_urls\n",
    "\n",
    "url_list3 = get_urls3(url_list[1:-12])\n",
    "\n",
    "regex = re.compile(\"https:\")\n",
    "filtered3 = [i for i in url_list3 if not regex.search(i)]\n",
    "\n",
    "pickling_on = open(\"url_page2.pkl\",\"wb\")\n",
    "pickle.dump(filtered3, pickling_on)\n",
    "pickling_on.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get poems!\n",
    "def get_poems(url):\n",
    "    list_of_poems = []\n",
    "    for element in filtered:\n",
    "        page = requests.get(f'{url}'+f'{element}')\n",
    "        data = page.text\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        \n",
    "        poem=soup.find_all('p')[1].get_text()\n",
    "            \n",
    "        list_of_poems.append(poem)\n",
    "        print(f'{element} has been added!')\n",
    "    return list_of_poems\n",
    "\n",
    "poems = get_poems('http://poemhunter.com/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping poetry 180\n",
    "<a id='180poems'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar code from above was used to scrape poetry 180, but was a lot simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T19:16:56.747204Z",
     "start_time": "2019-09-16T19:16:56.743134Z"
    }
   },
   "source": [
    "## Cleaning the Data\n",
    "<a id='clean'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T19:25:24.011783Z",
     "start_time": "2019-09-16T19:25:22.098155Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate my poems scraped from poemhunter.com and another website\n",
    "def master_poem(list_of_pkl):\n",
    "    empty_list = []\n",
    "    for element in list_of_pkl:\n",
    "        pickle_in = open(element,\"rb\")\n",
    "        pkl = pickle.load(pickle_in)\n",
    "        empty_list.extend(pkl)\n",
    "    return empty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating my poems. \n",
    "master = master_poem(['poems1.pkl', 'poems2.pkl', 'poems3.pkl', 'poems4.pkl', 'poems5.pkl', 'poems6.pkl', 'poems7.pkl',\n",
    "             'poems8.pkl', 'poems9.pkl', 'poems10.pkl', 'poems11.pkl', 'poems12.pkl', 'poems13.pkl', 'poems14.pkl',\n",
    "             'poems15.pkl', 'poems16.pkl', 'poems17.pkl', 'poems18.pkl', 'poems19.pkl', 'poems20.pkl',\n",
    "             'poems21.pkl', 'poems22.pkl', 'poems23.pkl', 'poems24.pkl', 'poems25.pkl', 'poems26.pkl',\n",
    "             'poems27.pkl', 'poems28.pkl', 'poems29.pkl', 'poems30.pkl', 'poems31.pkl', 'poems32.pkl',\n",
    "             'poetry_180_1.pkl', 'poetry_180_2.pkl', 'poetry_180_3.pkl', 'poetry_180_4.pkl'])\n",
    "\n",
    "#getting poems from kaggle dataset. \n",
    "df = pd.read_csv('kaggle_poem_dataset.csv')\n",
    "\n",
    "#creating list of kaggle poems.\n",
    "contentlist = []\n",
    "for element in df['Content']:\n",
    "    contentlist.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe of poems from poemhunter.com \n",
    "master_df = pd.DataFrame(flatten_master, columns = ['poem'])\n",
    "#creating dataframe of kaggle poems\n",
    "kaggle = pd.DataFrame(contentlist, columns = ['poem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating master dataframe of all poems\n",
    "master_df = pd.concat([master_df, kaggle], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean poems by removing non-english characters\n",
    "def clean_poems(poem):\n",
    "    return re.sub('[^ \\nA-Za-z!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]','',poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column of clean poems.\n",
    "master_df['poem_clean'] = master_df['poem'].apply(clean_poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates and drop index column. \n",
    "master_df['poem_clean'].drop_duplicates(inplace=True)\n",
    "master_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(master_df['poem_clean']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pickle file of master df of clean poems. \n",
    "with open('new_master2.pkl', 'wb') as handle:\n",
    "    pickle.dump(master_df, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 100 sequence chunks\n",
    "<a id='100'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script was edited and re-ran for every 500 poems, creating 43 sets of dataX and dataY files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import sys\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#load pickle file of cleaned data.\n",
    "pickle_in = open(\"new_master2.pkl\", \"rb\")\n",
    "df = pickle.load(pickle_in)\n",
    "\n",
    "#create list of clean poems.\n",
    "documents = []\n",
    "list1 = []\n",
    "for poem in df['poem_clean']:\n",
    "    documents.append(poem)\n",
    "    \n",
    "#tokenize the poems on the character level \n",
    "tk = Tokenizer(num_words= None, char_level=True, oov_token = 'UNK')\n",
    "#create dictionary with characters keys and their corresponding numerical values as values.\n",
    "tk.fit_on_texts(documents)\n",
    "\n",
    "\n",
    "#set sequence length to 100.\n",
    "seq_length = 100\n",
    "#create feature set- dataX, and target set - dataY\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "#run for every batch of 500 poems.\n",
    "for k in range (21000,21500):\n",
    "    #break up poems into 100 character chunks with a moving window of 3 characters.\n",
    "    for i in range (0, len(documents[k])-seq_length, 3):\n",
    "        sequences = tk.texts_to_sequences(documents[k])\n",
    "        \n",
    "        #getting 100 characters. \n",
    "        seq_in =documents[k][i: i + seq_length]\n",
    "        #getting the next character after the 100 character chunk.\n",
    "        seq_out = documents[k][i+seq_length]\n",
    "        \n",
    "        #append the 100 characters into dataX- features\n",
    "        dataX.append(sequences[i:i+seq_length])\n",
    "        #append the 101st, or the next character into dataY-target\n",
    "        dataY.append(sequences[i+seq_length])\n",
    "        \n",
    "#save the sequences and targets to pickle files.\n",
    "pickle_out = open(\"dataX_43.pkl\",\"wb\")\n",
    "pickle.dump(dataX, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(f\"dataY_43.pkl\", \"wb\")\n",
    "pickle.dump(dataY, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T19:48:35.568283Z",
     "start_time": "2019-09-16T19:48:35.559817Z"
    }
   },
   "source": [
    "## Create batches of 1,024 100 sequence chunks\n",
    "\n",
    "<a id='1024'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#create a list of files that will be read in a loop. \n",
    "#These are the files created in the 100 sequence chunks portion.\n",
    "pkl_files = []\n",
    "for n in range (1, 44):\n",
    "    pkl_files.append(f'dataX_{n}.pkl)\n",
    "\n",
    "#Since some file lengths are not divisible by 1024, remaining sequences will be stored in rem_slice\n",
    "rem_slice = None\n",
    "st = 0\n",
    "count = 0\n",
    "\n",
    "#loop through pkl_files and load them.\n",
    "for element in pkl_files:\n",
    "    pickle_in = open(element, \"rb\")\n",
    "    dataX = pickle.load(pickle_in)\n",
    "    \n",
    "    # if rem_slice is not None,the starting point where sequences are read for the next dataX file will be shifted.\n",
    "    # rem_slice sequences from the previous dataX gets added to the beginning of the current dataX file.\n",
    "    if rem_slice:\n",
    "        st = 1024 - len(rem_slice)\n",
    "        rem_slice.extend(dataX[:st])\n",
    "\n",
    "    #loop over 1024 sequences. \n",
    "    for y in range (st, len(dataX)-1024, 1024):\n",
    "        #append 1024 sequence batches to a file.\n",
    "        pickle_out = open(f\"batch_files3/dickinson_and_5000_X_{count}.pkl\", \"wb\")\n",
    "        pickle.dump(dataX[y:y+1024], pickle_out)\n",
    "        #counter keeps track of how many files are created and the naming of the files\n",
    "        count +=1\n",
    "                     \n",
    "    #the rem_slice collects remaining sequences. \n",
    "                    \n",
    "    rem_slice = dataX[y+1024:]\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same code, but for the targets.\n",
    "\n",
    "\n",
    "rem_slice = None\n",
    "st = 0\n",
    "count = 0\n",
    "\n",
    "#loop through pkl_files and load them.\n",
    "for element in pkl_files:\n",
    "    pickle_in = open(element, \"rb\")\n",
    "    dataY = pickle.load(pickle_in)\n",
    "    \n",
    "    # if rem_slice is not None,the starting point where sequences are read for the next dataX file will be shifted.\n",
    "    # rem_slice sequences from the previous dataX gets added to the beginning of the current dataX file.\n",
    "    if rem_slice:\n",
    "        st = 1024 - len(rem_slice)\n",
    "        rem_slice.extend(dataY[:st])\n",
    "\n",
    "    #loop over 1024 sequences. \n",
    "    for y in range (st, len(dataY)-1024, 1024):\n",
    "        #append 1024 sequence batches to a file.\n",
    "        pickle_out = open(f\"batch_files3/dickinson_and_5000_X_{count}.pkl\", \"wb\")\n",
    "        pickle.dump(dataY[y:y+1024], pickle_out)\n",
    "        #counter keeps track of how many files are created and the naming of the files\n",
    "        count +=1\n",
    "                     \n",
    "    #the rem_slice collects remaining sequences. \n",
    "                    \n",
    "    rem_slice = dataY[y+1024:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "<a id='lstm'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "import sys\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM\n",
    "#CuDNNLSTM can be used if using GPU to run code.\n",
    "#from keras.layers import CuDNNLSTM\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# function to load pkl files of 1024 100 character sequence chunks created previously\n",
    "def get_input(pklfile):\n",
    "    pickle_in = open(f\"batch_files3/{pklfile}.pkl\", \"rb\")\n",
    "    X = pickle.load(pickle_in)\n",
    "    return X\n",
    "\n",
    "#function to one hot encode sequences. \n",
    "def preprocess_input(X_or_Y):\n",
    "    X_or_Y_processed = np_utils.to_categorical(X_or_Y, num_classes=80)\n",
    "    return X_or_Y_processed\n",
    "\n",
    "#need to get the shape of X and y, features and targets.\n",
    "X= preprocess_input(get_input('X_2'))\n",
    "y= preprocess_input(get_input('Y_2'))\n",
    "\n",
    "#instantiate model\n",
    "model = Sequential()\n",
    "#256 memory units\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
    "#add dropout for regularization\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "#counter keeps track of how many batches were processed. \n",
    "counter = 0\n",
    "for n in range(1, 7507):\n",
    "    X = preprocess_input(get_input(f'X_{n}'))\n",
    "    y = preprocess_input(get_input(f'Y_{n}'))\n",
    "    \n",
    "    # early stopping prevents overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode = 'min', verbose=1)\n",
    "    callbacks = [es, ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    model.fit(X, y, epochs=20, batch_size=1024, callbacks=callbacks)\n",
    "    counter += 1\n",
    "    #save model for every 750 batches it processes\n",
    "    if counter % 750 == 0:\n",
    "        model.save(f\"model_weights_{counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poetry Generator\n",
    "<a id ='generator'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "import sys\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load master dataframe of poems.\n",
    "pickle_in = open(\"new_master2.pkl\", \"rb\")\n",
    "df = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a list of the clean poems. \n",
    "documents = []\n",
    "list1 = []\n",
    "for poem in df['poem_clean']:\n",
    "    documents.append(poem)\n",
    "    \n",
    "#tokenizing poems on the character level and getting the dictionary of keys and values, characters and corresponding numerical values\n",
    "tk = Tokenizer(num_words= None, char_level=True, oov_token = 'UNK')\n",
    "tk.fit_on_texts(documents)\n",
    "\n",
    "#see the dictionary\n",
    "tk.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to get the shape of targets and features.\n",
    "def get_input(pklfile):\n",
    "    pickle_in = open(pklfile, \"rb\")\n",
    "    X = pickle.load(pickle_in)\n",
    "    return X\n",
    "\n",
    "def preprocess_input(X_or_Y):\n",
    "    X_or_Y_processed = np_utils.to_categorical(X_or_Y, num_classes=80)\n",
    "    return X_or_Y_processed\n",
    "\n",
    "X= preprocess_input(get_input('batch_files3/X_1.pkl'))\n",
    "y= preprocess_input(get_input('batch_files3/Y_1.pkl'))\n",
    "\n",
    "#instantiate model\n",
    "model = Sequential()\n",
    "#use the same hyperparameters from our LSTM model.\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "#load the weights from saved model. we are using the model trained with 6000 1024 100 character chunks. \n",
    "model.load_weights(\"model_weights_6000\")\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted dictionary of word_index to translate numerical values to characters.\n",
    "inverted_dict = dict([[v,k] for k,v in tk.word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to set the temperature of poetry generation. \n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the seed for the poetry generator. to_transform can be edited.\n",
    "\n",
    "to_transform = \"anyones hand. \\\n",
    "i found the newborn sparrow next to \\\n",
    "the tumbled nest on the grass. bravely\\\n",
    "  \\\n",
    "opening it\"\n",
    "\n",
    "a = list(to_transform.lower()[0:100])\n",
    "\n",
    "pattern = []\n",
    "for element in a:\n",
    "    pattern.append(tk.word_index[element])\n",
    "    \n",
    "len(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate characters for different temperature ranges. \n",
    "for temperature in [.1, .3, .5, .7]:\n",
    "    print('----- temperature:', temperature)\n",
    "    for i in range(500):  \n",
    "        #reshapes x to fit into model.\n",
    "        x = np_utils.to_categorical(pattern, num_classes=80)\n",
    "        x = np.reshape(x, (1, 100, 80))\n",
    "\n",
    "        prediction=model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        index = sample(prediction, temperature)\n",
    "        #index = np.argmax(prediction)\n",
    "        \n",
    "        result=inverted_dict[index]\n",
    "        \n",
    "        seq_in = [inverted_dict[value] for value in pattern]\n",
    "        sys.stdout.write(result)\n",
    "        pattern.append(index)\n",
    "        pattern=pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
